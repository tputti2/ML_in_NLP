{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\trive\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'C:\\\\Users\\\\trive\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\c6f5954ffa065cdb2f2e604e740e8838bf21a2d3\\\\variables\\\\variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word = \"Elephant\"\\nsentence = \"I am a sentence for which I would like to get its embedding.\"\\nparagraph = (\\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\\n    \"the more \\'diluted\\' the embedding will be.\")\\nmessages = [word, sentence, paragraph]\\n\\n# Reduce logging output.\\ntf.logging.set_verbosity(tf.logging.ERROR)\\n\\nwith tf.Session() as session:\\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\\n    message_embeddings = session.run(embed(messages))\\n    \\n    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\\n        print(\"Message: {}\".format(messages[i]))\\n        print(\"Embedding size: {}\".format(len(message_embedding)))\\n        message_embedding_snippet = \", \".join(\\n            (str(x) for x in message_embedding[:3]))\\n        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\n",
    "'''word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "    \n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404287"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data.drop(['id','qid1','qid2'],axis = 1,inplace = True)\n",
    "#train_data['question1'][0],train_data['question2'][0]\n",
    "len(train_data)\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(train_data)\n",
    "#n = 10\n",
    "\n",
    "def plot_similarity(labels, feature1,feature2, rotation):\n",
    "    corr = np.inner(feature1,feature2)\n",
    "    return corr\n",
    "    #sns.set(font_scale=1.2)\n",
    "    #g = sns.heatmap(\n",
    "        #corr,\n",
    "        #xticklabels=labels,\n",
    "        #yticklabels=labels,\n",
    "        #vmin=0,\n",
    "        #vmax=1,\n",
    "        #cmap=\"YlOrRd\")\n",
    "    #g.set_xticklabels(labels, rotation=rotation)\n",
    "    #g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "    message_embeddings_ = session_.run(encoding_tensor, feed_dict={input_tensor_: messages})\n",
    "    #print(message_embeddings_[0])\n",
    "    corr_list = []\n",
    "    for i in range(0,2*n,2):\n",
    "        corr_list.append(plot_similarity(messages_, message_embeddings_[i],message_embeddings_[i+1], 90))\n",
    "    return (corr_list)\n",
    "\n",
    "def message_embeddings(session_, input_tensor_, messages_, encoding_tensor):\n",
    "    message_embeddings_ = session_.run(encoding_tensor, feed_dict={input_tensor_: messages})\n",
    "    #print(message_embeddings_[0])\n",
    "    embed = []\n",
    "    for i in range(0,2*n,2):\n",
    "        embed.append(np.concatenate([message_embeddings_[i],message_embeddings_[i+1]]))\n",
    "    return (embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)\n",
    "\n",
    "messages = []\n",
    "for i in range(n):\n",
    "    messages.append(train_data['question1'][i])\n",
    "    messages.append(train_data['question2'][i])\n",
    "    \n",
    "    \n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    feature = message_embeddings(session, similarity_input_placeholder, messages,\n",
    "                                  similarity_message_encodings)\n",
    "    #print(feature)\n",
    "    #print(feature2)\n",
    "    #print(corr_list)\n",
    "    \n",
    "print(time.time() - start)\n",
    "\n",
    "#plt.figure(figsize=(10,6))\n",
    "#plt.xlabel('Inner Product')\n",
    "#plt.ylabel('Semantic Similarity label (0 or 1)')\n",
    "#plt.scatter(corr_list,train_data['is_duplicate'][:n],c = train_data['is_duplicate'][:n])\n",
    "#plt.gray()\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 1025)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = pd.DataFrame(feature)\n",
    "check['label'] = train_data['is_duplicate'][:n]\n",
    "check.to_csv('Embeddings.csv')\n",
    "check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt(\"Embeddings_full.csv\",delimiter=',',skiprows=1)\n",
    "X = dataset[:,1:1025]\n",
    "\n",
    "Y = dataset[:,1025]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = np.zeros((len(X),512))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(512):\n",
    "        X_new[i][j] = X[i][j] - X[i][512+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1 = np.concatenate((X, X_new), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364287, 1536) (364287,)\n",
      "(40000, 1536) (40000,)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing vars\n",
    "validation_size = 40000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size = validation_size)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "#Y = np.array(train_data['is_duplicate'][:n])\n",
    "#Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364287 samples, validate on 40000 samples\n",
      "Epoch 1/50\n",
      "364287/364287 [==============================] - 468s 1ms/step - loss: 0.4580 - acc: 0.7704 - val_loss: 0.4383 - val_acc: 0.7834\n",
      "Epoch 2/50\n",
      "364287/364287 [==============================] - 172s 471us/step - loss: 0.4270 - acc: 0.7912 - val_loss: 0.4264 - val_acc: 0.7925\n",
      "Epoch 3/50\n",
      "364287/364287 [==============================] - 103s 284us/step - loss: 0.4177 - acc: 0.7973 - val_loss: 0.4251 - val_acc: 0.7931\n",
      "Epoch 4/50\n",
      "364287/364287 [==============================] - 66s 180us/step - loss: 0.4126 - acc: 0.8002 - val_loss: 0.4195 - val_acc: 0.7950\n",
      "Epoch 5/50\n",
      "364287/364287 [==============================] - 59s 161us/step - loss: 0.4084 - acc: 0.8028 - val_loss: 0.4225 - val_acc: 0.7959\n",
      "Epoch 6/50\n",
      "364287/364287 [==============================] - 59s 161us/step - loss: 0.4057 - acc: 0.8045 - val_loss: 0.4177 - val_acc: 0.7957\n",
      "Epoch 7/50\n",
      "364287/364287 [==============================] - 60s 165us/step - loss: 0.4030 - acc: 0.8064 - val_loss: 0.4140 - val_acc: 0.7984\n",
      "Epoch 8/50\n",
      "364287/364287 [==============================] - 59s 163us/step - loss: 0.4012 - acc: 0.8073 - val_loss: 0.4133 - val_acc: 0.7993\n",
      "Epoch 9/50\n",
      "364287/364287 [==============================] - 60s 165us/step - loss: 0.3993 - acc: 0.8086 - val_loss: 0.4152 - val_acc: 0.7984\n",
      "Epoch 10/50\n",
      "364287/364287 [==============================] - 59s 162us/step - loss: 0.3977 - acc: 0.8099 - val_loss: 0.4135 - val_acc: 0.7995\n",
      "Epoch 11/50\n",
      "364287/364287 [==============================] - 60s 165us/step - loss: 0.3965 - acc: 0.8110 - val_loss: 0.4208 - val_acc: 0.7992\n",
      "Epoch 12/50\n",
      "364287/364287 [==============================] - 61s 167us/step - loss: 0.3951 - acc: 0.8114 - val_loss: 0.4116 - val_acc: 0.8008\n",
      "Epoch 13/50\n",
      "364287/364287 [==============================] - 60s 165us/step - loss: 0.3939 - acc: 0.8121 - val_loss: 0.4111 - val_acc: 0.8019\n",
      "Epoch 14/50\n",
      "364287/364287 [==============================] - 61s 167us/step - loss: 0.3928 - acc: 0.8122 - val_loss: 0.4135 - val_acc: 0.7993\n",
      "Epoch 15/50\n",
      "364287/364287 [==============================] - 61s 166us/step - loss: 0.3917 - acc: 0.8138 - val_loss: 0.4108 - val_acc: 0.8019\n",
      "Epoch 16/50\n",
      "364287/364287 [==============================] - 61s 168us/step - loss: 0.3907 - acc: 0.8141 - val_loss: 0.4100 - val_acc: 0.8010\n",
      "Epoch 17/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3900 - acc: 0.8148 - val_loss: 0.4101 - val_acc: 0.8011\n",
      "Epoch 18/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3892 - acc: 0.8147 - val_loss: 0.4118 - val_acc: 0.8026\n",
      "Epoch 19/50\n",
      "364287/364287 [==============================] - 61s 168us/step - loss: 0.3887 - acc: 0.8150 - val_loss: 0.4098 - val_acc: 0.8014\n",
      "Epoch 20/50\n",
      "364287/364287 [==============================] - 61s 168us/step - loss: 0.3879 - acc: 0.8160 - val_loss: 0.4101 - val_acc: 0.8019\n",
      "Epoch 21/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3875 - acc: 0.8166 - val_loss: 0.4173 - val_acc: 0.7951\n",
      "Epoch 22/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3867 - acc: 0.8170 - val_loss: 0.4106 - val_acc: 0.8033\n",
      "Epoch 23/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3868 - acc: 0.8169 - val_loss: 0.4091 - val_acc: 0.8015\n",
      "Epoch 24/50\n",
      "364287/364287 [==============================] - 62s 171us/step - loss: 0.3860 - acc: 0.8169 - val_loss: 0.4092 - val_acc: 0.8019\n",
      "Epoch 25/50\n",
      "364287/364287 [==============================] - 62s 169us/step - loss: 0.3852 - acc: 0.8175 - val_loss: 0.4125 - val_acc: 0.8032\n",
      "Epoch 26/50\n",
      "364287/364287 [==============================] - 64s 174us/step - loss: 0.3849 - acc: 0.8177 - val_loss: 0.4109 - val_acc: 0.8041\n",
      "Epoch 27/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3843 - acc: 0.8183 - val_loss: 0.4131 - val_acc: 0.8017\n",
      "Epoch 28/50\n",
      "364287/364287 [==============================] - 62s 169us/step - loss: 0.3841 - acc: 0.8188 - val_loss: 0.4121 - val_acc: 0.8027\n",
      "Epoch 29/50\n",
      "364287/364287 [==============================] - 62s 171us/step - loss: 0.3839 - acc: 0.8187 - val_loss: 0.4117 - val_acc: 0.8018\n",
      "Epoch 30/50\n",
      "364287/364287 [==============================] - 63s 172us/step - loss: 0.3836 - acc: 0.8185 - val_loss: 0.4104 - val_acc: 0.8037\n",
      "Epoch 31/50\n",
      "364287/364287 [==============================] - 63s 172us/step - loss: 0.3832 - acc: 0.8190 - val_loss: 0.4091 - val_acc: 0.8032\n",
      "Epoch 32/50\n",
      "364287/364287 [==============================] - 65s 178us/step - loss: 0.3833 - acc: 0.8190 - val_loss: 0.4121 - val_acc: 0.8043\n",
      "Epoch 33/50\n",
      "364287/364287 [==============================] - 63s 173us/step - loss: 0.3826 - acc: 0.8196 - val_loss: 0.4089 - val_acc: 0.8018\n",
      "Epoch 34/50\n",
      "364287/364287 [==============================] - 62s 171us/step - loss: 0.3826 - acc: 0.8194 - val_loss: 0.4085 - val_acc: 0.8032\n",
      "Epoch 35/50\n",
      "364287/364287 [==============================] - 62s 170us/step - loss: 0.3826 - acc: 0.8195 - val_loss: 0.4147 - val_acc: 0.8017\n",
      "Epoch 36/50\n",
      "364287/364287 [==============================] - 63s 172us/step - loss: 0.3824 - acc: 0.8191 - val_loss: 0.4127 - val_acc: 0.8025\n",
      "Epoch 37/50\n",
      "364287/364287 [==============================] - 63s 173us/step - loss: 0.3820 - acc: 0.8201 - val_loss: 0.4101 - val_acc: 0.8040\n",
      "Epoch 38/50\n",
      "364287/364287 [==============================] - 64s 177us/step - loss: 0.3825 - acc: 0.8202 - val_loss: 0.4112 - val_acc: 0.8007\n",
      "Epoch 39/50\n",
      "364287/364287 [==============================] - 85s 234us/step - loss: 0.3823 - acc: 0.8203 - val_loss: 0.4114 - val_acc: 0.8010\n",
      "Epoch 40/50\n",
      "364287/364287 [==============================] - 93s 255us/step - loss: 0.3820 - acc: 0.8205 - val_loss: 0.4098 - val_acc: 0.8032\n",
      "Epoch 41/50\n",
      "364287/364287 [==============================] - 93s 255us/step - loss: 0.3813 - acc: 0.8203 - val_loss: 0.4182 - val_acc: 0.8005\n",
      "Epoch 42/50\n",
      "364287/364287 [==============================] - 92s 253us/step - loss: 0.3816 - acc: 0.8209 - val_loss: 0.4114 - val_acc: 0.8034\n",
      "Epoch 43/50\n",
      "364287/364287 [==============================] - 91s 250us/step - loss: 0.3818 - acc: 0.8207 - val_loss: 0.4114 - val_acc: 0.8033\n",
      "Epoch 44/50\n",
      "364287/364287 [==============================] - 91s 249us/step - loss: 0.3818 - acc: 0.8205 - val_loss: 0.4129 - val_acc: 0.8040\n",
      "Epoch 45/50\n",
      "364287/364287 [==============================] - 91s 249us/step - loss: 0.3819 - acc: 0.8206 - val_loss: 0.4154 - val_acc: 0.8000\n",
      "Epoch 46/50\n",
      "364287/364287 [==============================] - 91s 250us/step - loss: 0.3812 - acc: 0.8212 - val_loss: 0.4156 - val_acc: 0.8005\n",
      "Epoch 47/50\n",
      "364287/364287 [==============================] - 90s 246us/step - loss: 0.3820 - acc: 0.8212 - val_loss: 0.4164 - val_acc: 0.8015\n",
      "Epoch 48/50\n",
      "364287/364287 [==============================] - 89s 244us/step - loss: 0.3836 - acc: 0.8207 - val_loss: 0.4120 - val_acc: 0.8040\n",
      "Epoch 49/50\n",
      "364287/364287 [==============================] - 89s 244us/step - loss: 0.3815 - acc: 0.8210 - val_loss: 0.4100 - val_acc: 0.8019\n",
      "Epoch 50/50\n",
      "364287/364287 [==============================] - 88s 243us/step - loss: 0.3811 - acc: 0.8213 - val_loss: 0.4099 - val_acc: 0.8045\n",
      "364287/364287 [==============================] - 19s 51us/step\n",
      "\n",
      "acc: 82.50%\n",
      "Test-Accuracy: 0.800623496769\n",
      "4007.2682456970215\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=1536, activation='relu'))\n",
    "model.add(Dense(30, activation='relu')) #tanh Bruteforce for activation function\n",
    "model.add(Dense(30, activation='relu')) #tanh Bruteforce for activation function\n",
    "model.add(Dense(30, activation='relu')) #tanh Bruteforce for activation function\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "result = model.fit(X_train, y_train, epochs=50, batch_size=10,validation_data = (X_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(\"Test-Accuracy:\", np.mean(result.history[\"val_acc\"]))\n",
    "\n",
    "# calculate predictions\n",
    "#predictions = model.predict(X_test)\n",
    "# round predictions\n",
    "#rounded = [round(x[0]) for x in predictions]\n",
    "#print(rounded)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
